{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae66472",
   "metadata": {},
   "source": [
    "# Logistic regression using vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5930bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import paskages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4021d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, learning_rate, num_iters):\n",
    "    \"\"\"\n",
    "    Logistic regression implementation with vectorization.\n",
    "\n",
    "    Args:\n",
    "        X: A numpy array of shape (m, n) where m is the number of training examples and\n",
    "           n is the number of features.\n",
    "        y: A numpy array of shape (m,) containing the labels (0 or 1) for each training example.\n",
    "        learning_rate: The learning rate parameter.\n",
    "        num_iters: The number of iterations to run the gradient descent algorithm.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the learned weights (w) and bias (b).\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # Number of training examples and features\n",
    "    w = np.zeros((n,))  # Initialize weights to zeros\n",
    "    b = 0  # Initialize bias to zero\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Compute linear combination of weights and features plus bias\n",
    "        z = X.dot(w) + b\n",
    "        # Apply sigmoid activation function to get probabilities\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        # Compute the difference between predicted and true values\n",
    "        dz = a - y\n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = (X.T @ dz) / m  # Gradient of weights\n",
    "        db = np.sum(dz) / m  # Gradient of bias\n",
    "        \n",
    "        # Update weights and bias using gradients and learning rate\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "    # Return learned weights and bias\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd29e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57eefc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "X= np.array([[20,70],[23,130],[30,90],[45,50]])\n",
    "y=np.array([1,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de5e7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8fffb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b= logistic_regression(X,y ,learning_rate , num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d51d09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: [-0.02172353  0.01421215]\n",
      "Learned bias: 0.010175684717251962\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learned weights: {w}\")\n",
    "print(f\"Learned bias: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcb6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
